{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ef929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 30 11:58:24 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   19C    P8     9W / 250W |      0MiB / 23040MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P40           On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   22C    P8     9W / 250W |      0MiB / 23040MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c85afaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc55b6a6af8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "import os\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def seed_everything(seed=12):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "RANDOM_SEED = 12\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d95d6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/processed_wikiart_title.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786e6536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Token count')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "token_lens = []\n",
    "\n",
    "for txt in df.Title:\n",
    "    tokens = tokenizer.encode(txt, max_length=30)\n",
    "    token_lens.append(len(tokens))\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 30]);\n",
    "plt.xlabel('Token count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8f3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "class Wikiart_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, title, targets, tokenizer, max_len):\n",
    "        self.title = title\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "  \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.title[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          review,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=True,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          \n",
    "          return_tensors='pt',\n",
    "        )\n",
    "        ids = encoding['input_ids']\n",
    "        mask = encoding['attention_mask']\n",
    "       \n",
    "        \n",
    "        return {\n",
    "          \n",
    "          'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "           'mask': torch.tensor(mask, dtype=torch.long),\n",
    "           'targets': torch.tensor(self.targets[item], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39a3a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33e91254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'mask', 'targets'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = Wikiart_Dataset(\n",
    "    title=df.Title.to_numpy(),\n",
    "    targets=df.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "    return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2\n",
    "  )\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "data = next(iter(train_data_loader))\n",
    "data.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6689b2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "n_classes=6\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=n_classes).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c1e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 6\n",
    "class CE_LS(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, classes= num_classes, smoothing=0, ignore_index=-1):\n",
    "        super(CE_LS, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.complement = 1.0 - smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            oh_labels = F.one_hot(target.to(torch.int64), num_classes = self.cls).contiguous()\n",
    "            smoothen_ohlabel = oh_labels * self.complement + self.smoothing / self.cls\n",
    "        \n",
    "        logs = self.log_softmax(logits[target!=self.ignore_index])\n",
    "        return -torch.sum(logs * smoothen_ohlabel[target!=self.ignore_index], dim=1).mean()\n",
    "    \n",
    "\n",
    "loss_fn = CE_LS(classes = num_classes).to(device)\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = d[\"mask\"].squeeze(1).to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets.long())\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask = d[\"mask\"].squeeze(1).to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                ).logits\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets.long())\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60efb462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "Train loss 1.0680078192821985 accuracy 0.6108404384896468\n",
      "Val loss 0.9647110998630524 accuracy 0.6406820950060901\n",
      "\n",
      "epoch: 0  acc: 0.6407  best epoch: 0  best acc: 0.6407 lr: 0.0000\n",
      "Epoch 2/100\n",
      "----------\n",
      "Train loss 0.7119111629944403 accuracy 0.7591352009744214\n",
      "Val loss 0.9909713268280029 accuracy 0.6552984165651644\n",
      "\n",
      "epoch: 1  acc: 0.6553  best epoch: 1  best acc: 0.6553 lr: 0.0000\n",
      "Epoch 3/100\n",
      "----------\n",
      "Train loss 0.49254523391283833 accuracy 0.843483556638246\n",
      "Val loss 1.0483484772535472 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 2  acc: 0.6784  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 4/100\n",
      "----------\n",
      "Train loss 0.33365617432061906 accuracy 0.8934226552984165\n",
      "Val loss 1.241142282119164 accuracy 0.6577344701583434\n",
      "\n",
      "epoch: 3  acc: 0.6577  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 5/100\n",
      "----------\n",
      "Train loss 0.24269464800080048 accuracy 0.9174786845310596\n",
      "Val loss 1.3088814524503856 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 4  acc: 0.6724  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 6/100\n",
      "----------\n",
      "Train loss 0.1791154610861129 accuracy 0.9457978075517661\n",
      "Val loss 1.4437638291945825 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 5  acc: 0.6687  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 7/100\n",
      "----------\n",
      "Train loss 0.14348713424309945 accuracy 0.955237515225335\n",
      "Val loss 1.5411674357377565 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 6  acc: 0.6736  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 8/100\n",
      "----------\n",
      "Train loss 0.11809632152734885 accuracy 0.9619366626065773\n",
      "Val loss 1.626388662136518 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 7  acc: 0.6711  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 9/100\n",
      "----------\n",
      "Train loss 0.10587674507970732 accuracy 0.9646772228989037\n",
      "Val loss 1.6587906571534963 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 8  acc: 0.6736  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 10/100\n",
      "----------\n",
      "Train loss 0.08650362138744914 accuracy 0.970462850182704\n",
      "Val loss 1.8145462503800025 accuracy 0.6674786845310596\n",
      "\n",
      "epoch: 9  acc: 0.6675  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 11/100\n",
      "----------\n",
      "Train loss 0.08096274882509798 accuracy 0.9744214372716199\n",
      "Val loss 1.8507454074346101 accuracy 0.6772228989037758\n",
      "\n",
      "epoch: 10  acc: 0.6772  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 12/100\n",
      "----------\n",
      "Train loss 0.07342305745097619 accuracy 0.9744214372716199\n",
      "Val loss 1.8656339599536016 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 11  acc: 0.6663  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 13/100\n",
      "----------\n",
      "Train loss 0.07345826909606101 accuracy 0.9750304506699147\n",
      "Val loss 1.873597278044774 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 12  acc: 0.6711  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 14/100\n",
      "----------\n",
      "Train loss 0.0684155830898711 accuracy 0.9762484774665042\n",
      "Val loss 1.911631519977863 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 13  acc: 0.6699  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 15/100\n",
      "----------\n",
      "Train loss 0.06251782559093486 accuracy 0.976857490864799\n",
      "Val loss 1.946844729093405 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 14  acc: 0.6724  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 16/100\n",
      "----------\n",
      "Train loss 0.062581535955457 accuracy 0.9774665042630938\n",
      "Val loss 1.9912447516734784 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 15  acc: 0.6724  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 17/100\n",
      "----------\n",
      "Train loss 0.0612570114193023 accuracy 0.9774665042630938\n",
      "Val loss 2.027130126953125 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 16  acc: 0.6711  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 18/100\n",
      "----------\n",
      "Train loss 0.05912326809269765 accuracy 0.9771619975639464\n",
      "Val loss 2.042526533970466 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 17  acc: 0.6760  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 19/100\n",
      "----------\n",
      "Train loss 0.057857580165925786 accuracy 0.976857490864799\n",
      "Val loss 2.0817986772610593 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 18  acc: 0.6736  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 20/100\n",
      "----------\n",
      "Train loss 0.054662752836079166 accuracy 0.9783800243605358\n",
      "Val loss 2.1179225261394796 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 19  acc: 0.6699  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 21/100\n",
      "----------\n",
      "Train loss 0.059853127341518556 accuracy 0.976857490864799\n",
      "Val loss 2.1138862325594974 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 20  acc: 0.6736  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 22/100\n",
      "----------\n",
      "Train loss 0.05505519943510734 accuracy 0.9777710109622411\n",
      "Val loss 2.1381368820483866 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 21  acc: 0.6784  best epoch: 2  best acc: 0.6784 lr: 0.0000\n",
      "Epoch 23/100\n",
      "----------\n",
      "Train loss 0.0527857897994764 accuracy 0.9771619975639464\n",
      "Val loss 2.133755949827341 accuracy 0.682095006090134\n",
      "\n",
      "epoch: 22  acc: 0.6821  best epoch: 22  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 24/100\n",
      "----------\n",
      "Train loss 0.05535370762374035 accuracy 0.9795980511571254\n",
      "Val loss 2.1828617866222677 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 23  acc: 0.6784  best epoch: 22  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 25/100\n",
      "----------\n",
      "Train loss 0.05325460045946563 accuracy 0.9759439707673568\n",
      "Val loss 2.210959012691791 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 24  acc: 0.6687  best epoch: 22  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 26/100\n",
      "----------\n",
      "Train loss 0.057696430611331465 accuracy 0.9750304506699147\n",
      "Val loss 2.17827438391172 accuracy 0.6772228989037758\n",
      "\n",
      "epoch: 25  acc: 0.6772  best epoch: 22  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 27/100\n",
      "----------\n",
      "Train loss 0.05235742037953103 accuracy 0.9780755176613884\n",
      "Val loss 2.2082956433296204 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 26  acc: 0.6784  best epoch: 22  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 28/100\n",
      "----------\n",
      "Train loss 0.05216399744154153 accuracy 0.9777710109622411\n",
      "Val loss 2.247907349696526 accuracy 0.684531059683313\n",
      "\n",
      "epoch: 27  acc: 0.6845  best epoch: 27  best acc: 0.6845 lr: 0.0000\n",
      "Epoch 29/100\n",
      "----------\n",
      "Train loss 0.052061495611622696 accuracy 0.9799025578562728\n",
      "Val loss 2.260503337933467 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 28  acc: 0.6748  best epoch: 27  best acc: 0.6845 lr: 0.0000\n",
      "Epoch 30/100\n",
      "----------\n",
      "Train loss 0.050999131887525015 accuracy 0.9780755176613884\n",
      "Val loss 2.275127126620366 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 29  acc: 0.6724  best epoch: 27  best acc: 0.6845 lr: 0.0000\n",
      "Epoch 31/100\n",
      "----------\n",
      "Train loss 0.0498257914079959 accuracy 0.9777710109622411\n",
      "Val loss 2.3032975930434008 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 30  acc: 0.6736  best epoch: 27  best acc: 0.6845 lr: 0.0000\n",
      "Epoch 32/100\n",
      "----------\n",
      "Train loss 0.04856385740545325 accuracy 0.9786845310596832\n",
      "Val loss 2.354005304666666 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 31  acc: 0.6736  best epoch: 27  best acc: 0.6845 lr: 0.0000\n",
      "Epoch 33/100\n",
      "----------\n",
      "Train loss 0.0493202212326527 accuracy 0.9774665042630938\n",
      "Val loss 2.3092232025586643 accuracy 0.6808769792935444\n",
      "\n",
      "epoch: 32  acc: 0.6809  best epoch: 27  best acc: 0.6845 lr: 0.0000\n",
      "Epoch 34/100\n",
      "----------\n",
      "Train loss 0.04902010510386831 accuracy 0.9774665042630938\n",
      "Val loss 2.3201325306525598 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 33  acc: 0.6784  best epoch: 27  best acc: 0.6845 lr: 0.0000\n",
      "Epoch 35/100\n",
      "----------\n",
      "Train loss 0.048913101555194106 accuracy 0.9780755176613884\n",
      "Val loss 2.332471572435819 accuracy 0.6674786845310596\n",
      "\n",
      "epoch: 34  acc: 0.6675  best epoch: 27  best acc: 0.6845 lr: 0.0000\n",
      "Epoch 36/100\n",
      "----------\n",
      "Train loss 0.048643994652293174 accuracy 0.979293544457978\n",
      "Val loss 2.353196542996627 accuracy 0.6881851400730816\n",
      "\n",
      "epoch: 35  acc: 0.6882  best epoch: 35  best acc: 0.6882 lr: 0.0000\n",
      "Epoch 37/100\n",
      "----------\n",
      "Train loss 0.049017487488120785 accuracy 0.9777710109622411\n",
      "Val loss 2.3568103405145497 accuracy 0.6906211936662606\n",
      "\n",
      "epoch: 36  acc: 0.6906  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 38/100\n",
      "----------\n",
      "Train loss 0.04777524196720813 accuracy 0.9795980511571254\n",
      "Val loss 2.402452922784365 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 37  acc: 0.6760  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 39/100\n",
      "----------\n",
      "Train loss 0.04975343794444377 accuracy 0.979293544457978\n",
      "Val loss 2.421008573128627 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 38  acc: 0.6784  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 40/100\n",
      "----------\n",
      "Train loss 0.04879355464792964 accuracy 0.9789890377588306\n",
      "Val loss 2.388552949978755 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 39  acc: 0.6687  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 41/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.05103826211712852 accuracy 0.9799025578562728\n",
      "Val loss 2.3822086361738353 accuracy 0.6833130328867235\n",
      "\n",
      "epoch: 40  acc: 0.6833  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 42/100\n",
      "----------\n",
      "Train loss 0.04874038829658072 accuracy 0.9771619975639464\n",
      "Val loss 2.365232366781968 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 41  acc: 0.6748  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 43/100\n",
      "----------\n",
      "Train loss 0.04818117948098095 accuracy 0.9783800243605358\n",
      "Val loss 2.4240021247130175 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 42  acc: 0.6760  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 44/100\n",
      "----------\n",
      "Train loss 0.0475235561699238 accuracy 0.9783800243605358\n",
      "Val loss 2.4721931035702047 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 43  acc: 0.6784  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 45/100\n",
      "----------\n",
      "Train loss 0.0480162465930193 accuracy 0.9783800243605358\n",
      "Val loss 2.462191567971156 accuracy 0.6833130328867235\n",
      "\n",
      "epoch: 44  acc: 0.6833  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 46/100\n",
      "----------\n",
      "Train loss 0.04740313814277083 accuracy 0.9789890377588306\n",
      "Val loss 2.466161439051995 accuracy 0.6796589524969548\n",
      "\n",
      "epoch: 45  acc: 0.6797  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 47/100\n",
      "----------\n",
      "Train loss 0.04757839517845338 accuracy 0.9789890377588306\n",
      "Val loss 2.45861050257316 accuracy 0.6808769792935444\n",
      "\n",
      "epoch: 46  acc: 0.6809  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 48/100\n",
      "----------\n",
      "Train loss 0.04569781394870341 accuracy 0.9802070645554202\n",
      "Val loss 2.469935628084036 accuracy 0.6808769792935444\n",
      "\n",
      "epoch: 47  acc: 0.6809  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 49/100\n",
      "----------\n",
      "Train loss 0.047362771035269244 accuracy 0.9786845310596832\n",
      "Val loss 2.4772928678072414 accuracy 0.6796589524969548\n",
      "\n",
      "epoch: 48  acc: 0.6797  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 50/100\n",
      "----------\n",
      "Train loss 0.04678106893592026 accuracy 0.9777710109622411\n",
      "Val loss 2.4814052948584924 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 49  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 51/100\n",
      "----------\n",
      "Train loss 0.04661346918542178 accuracy 0.9777710109622411\n",
      "Val loss 2.5072880524855394 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 50  acc: 0.6687  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 52/100\n",
      "----------\n",
      "Train loss 0.04651650035026533 accuracy 0.9805115712545676\n",
      "Val loss 2.507688678227938 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 51  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 53/100\n",
      "----------\n",
      "Train loss 0.04731784452423137 accuracy 0.9786845310596832\n",
      "Val loss 2.514145085444817 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 52  acc: 0.6760  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 54/100\n",
      "----------\n",
      "Train loss 0.04768401707688717 accuracy 0.979293544457978\n",
      "Val loss 2.5402033649958096 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 53  acc: 0.6784  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 55/100\n",
      "----------\n",
      "Train loss 0.04607263641125066 accuracy 0.9789890377588306\n",
      "Val loss 2.556438670708583 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 54  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 56/100\n",
      "----------\n",
      "Train loss 0.046852448716555105 accuracy 0.9802070645554202\n",
      "Val loss 2.540780489261334 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 55  acc: 0.6736  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 57/100\n",
      "----------\n",
      "Train loss 0.04569194075403943 accuracy 0.9805115712545676\n",
      "Val loss 2.549274206161499 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 56  acc: 0.6748  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 58/100\n",
      "----------\n",
      "Train loss 0.04531575767908277 accuracy 0.9802070645554202\n",
      "Val loss 2.5723466919018674 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 57  acc: 0.6748  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 59/100\n",
      "----------\n",
      "Train loss 0.04697468448560278 accuracy 0.9795980511571254\n",
      "Val loss 2.572299755536593 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 58  acc: 0.6748  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 60/100\n",
      "----------\n",
      "Train loss 0.04623907435661661 accuracy 0.9795980511571254\n",
      "Val loss 2.5658352558429423 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 59  acc: 0.6784  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 61/100\n",
      "----------\n",
      "Train loss 0.04708425402138337 accuracy 0.9777710109622411\n",
      "Val loss 2.5966337827535777 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 60  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 62/100\n",
      "----------\n",
      "Train loss 0.04799393142958667 accuracy 0.9786845310596832\n",
      "Val loss 2.5952952733406653 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 61  acc: 0.6760  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 63/100\n",
      "----------\n",
      "Train loss 0.04520979749517192 accuracy 0.9805115712545676\n",
      "Val loss 2.5847176175851088 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 62  acc: 0.6784  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 64/100\n",
      "----------\n",
      "Train loss 0.045651181498418104 accuracy 0.9795980511571254\n",
      "Val loss 2.5950769369418802 accuracy 0.6674786845310596\n",
      "\n",
      "epoch: 63  acc: 0.6675  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 65/100\n",
      "----------\n",
      "Train loss 0.04626578714259735 accuracy 0.9795980511571254\n",
      "Val loss 2.5747674795297475 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 64  acc: 0.6663  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 66/100\n",
      "----------\n",
      "Train loss 0.04674668258165678 accuracy 0.9789890377588306\n",
      "Val loss 2.5976258516311646 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 65  acc: 0.6699  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 67/100\n",
      "----------\n",
      "Train loss 0.046093638203441936 accuracy 0.979293544457978\n",
      "Val loss 2.62940985422868 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 66  acc: 0.6663  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 68/100\n",
      "----------\n",
      "Train loss 0.045424745958812705 accuracy 0.9805115712545676\n",
      "Val loss 2.6245951056480408 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 67  acc: 0.6784  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 69/100\n",
      "----------\n",
      "Train loss 0.045478242177285085 accuracy 0.9789890377588306\n",
      "Val loss 2.6312477542803836 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 68  acc: 0.6748  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 70/100\n",
      "----------\n",
      "Train loss 0.04559779946868324 accuracy 0.9799025578562728\n",
      "Val loss 2.6402634244698744 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 69  acc: 0.6760  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 71/100\n",
      "----------\n",
      "Train loss 0.044842497322940635 accuracy 0.9799025578562728\n",
      "Val loss 2.6258942668254557 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 70  acc: 0.6736  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 72/100\n",
      "----------\n",
      "Train loss 0.04473105307458787 accuracy 0.9795980511571254\n",
      "Val loss 2.638667207497817 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 71  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 73/100\n",
      "----------\n",
      "Train loss 0.04650354576952647 accuracy 0.9795980511571254\n",
      "Val loss 2.6452437960184536 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 72  acc: 0.6699  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 74/100\n",
      "----------\n",
      "Train loss 0.044662135840253456 accuracy 0.980816077953715\n",
      "Val loss 2.6391847271185656 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 73  acc: 0.6711  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 75/100\n",
      "----------\n",
      "Train loss 0.04476622520521841 accuracy 0.9805115712545676\n",
      "Val loss 2.6470522926403928 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 74  acc: 0.6687  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 76/100\n",
      "----------\n",
      "Train loss 0.04501579372914173 accuracy 0.9820341047503045\n",
      "Val loss 2.6655490215008077 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 75  acc: 0.6711  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 77/100\n",
      "----------\n",
      "Train loss 0.04509346973052355 accuracy 0.9783800243605358\n",
      "Val loss 2.657537419062394 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 76  acc: 0.6711  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 78/100\n",
      "----------\n",
      "Train loss 0.04505424398716394 accuracy 0.9795980511571254\n",
      "Val loss 2.6572679923130917 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 77  acc: 0.6699  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 79/100\n",
      "----------\n",
      "Train loss 0.04574668037652002 accuracy 0.9814250913520097\n",
      "Val loss 2.6586837585155783 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 78  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 80/100\n",
      "----------\n",
      "Train loss 0.04516416242771705 accuracy 0.9805115712545676\n",
      "Val loss 2.666070364988767 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 79  acc: 0.6711  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 81/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.04442211374880924 accuracy 0.9817295980511571\n",
      "Val loss 2.669325136221372 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 80  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 82/100\n",
      "----------\n",
      "Train loss 0.04458962549730679 accuracy 0.9802070645554202\n",
      "Val loss 2.6560677656760583 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 81  acc: 0.6760  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 83/100\n",
      "----------\n",
      "Train loss 0.044209330907055894 accuracy 0.9805115712545676\n",
      "Val loss 2.6692130978290853 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 82  acc: 0.6687  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 84/100\n",
      "----------\n",
      "Train loss 0.04411761909757189 accuracy 0.9817295980511571\n",
      "Val loss 2.6703783181997447 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 83  acc: 0.6736  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 85/100\n",
      "----------\n",
      "Train loss 0.044450916728449565 accuracy 0.9795980511571254\n",
      "Val loss 2.670957721196688 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 84  acc: 0.6711  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 86/100\n",
      "----------\n",
      "Train loss 0.04515483894371834 accuracy 0.9811205846528623\n",
      "Val loss 2.6678171203686643 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 85  acc: 0.6748  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 87/100\n",
      "----------\n",
      "Train loss 0.04372910874569676 accuracy 0.9811205846528623\n",
      "Val loss 2.6678852301377516 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 86  acc: 0.6748  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 88/100\n",
      "----------\n",
      "Train loss 0.04428246955684549 accuracy 0.9799025578562728\n",
      "Val loss 2.6666604463870707 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 87  acc: 0.6748  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 89/100\n",
      "----------\n",
      "Train loss 0.045026870067833215 accuracy 0.979293544457978\n",
      "Val loss 2.6708388374401975 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 88  acc: 0.6736  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 90/100\n",
      "----------\n",
      "Train loss 0.04391749980348022 accuracy 0.9820341047503045\n",
      "Val loss 2.6757851380568285 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 89  acc: 0.6711  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 91/100\n",
      "----------\n",
      "Train loss 0.0441208201210756 accuracy 0.979293544457978\n",
      "Val loss 2.6802979111671448 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 90  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 92/100\n",
      "----------\n",
      "Train loss 0.045081960018842876 accuracy 0.9814250913520097\n",
      "Val loss 2.6800900376760044 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 91  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 93/100\n",
      "----------\n",
      "Train loss 0.0449230488705069 accuracy 0.979293544457978\n",
      "Val loss 2.6803944202569814 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 92  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 94/100\n",
      "----------\n",
      "Train loss 0.04552112632398313 accuracy 0.9799025578562728\n",
      "Val loss 2.6754198166040273 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 93  acc: 0.6711  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 95/100\n",
      "----------\n",
      "Train loss 0.04359472995325561 accuracy 0.9820341047503045\n",
      "Val loss 2.676971747325017 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 94  acc: 0.6736  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 96/100\n",
      "----------\n",
      "Train loss 0.04478106824281338 accuracy 0.9789890377588306\n",
      "Val loss 2.678936848273644 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 95  acc: 0.6711  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 97/100\n",
      "----------\n",
      "Train loss 0.04401726241972867 accuracy 0.9814250913520097\n",
      "Val loss 2.6794754633536706 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 96  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 98/100\n",
      "----------\n",
      "Train loss 0.04451053088389052 accuracy 0.9814250913520097\n",
      "Val loss 2.6790430408257704 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 97  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 99/100\n",
      "----------\n",
      "Train loss 0.04370649799698053 accuracy 0.9814250913520097\n",
      "Val loss 2.679197031718034 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 98  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n",
      "Epoch 100/100\n",
      "----------\n",
      "Train loss 0.044635376344037636 accuracy 0.9795980511571254\n",
      "Val loss 2.67949961240475 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 99  acc: 0.6724  best epoch: 36  best acc: 0.6906 lr: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed_everything()\n",
    "EPOCHS = 100\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "            \n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model,train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model,test_data_loader,loss_fn, device, len(df_test))\n",
    "\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_wikiart_title_cls_transf.pth')\n",
    "        best_accuracy = val_acc\n",
    "        best_epoch = epoch\n",
    "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f} lr: {:.4f}'.format(\n",
    "            epoch, val_acc, best_epoch, best_accuracy,optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971c4d4",
   "metadata": {},
   "source": [
    "Confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29edfea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8167, 0.8399, 0.8422, 0.8287, 0.7249, 0.8400], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_conf_freq (model, dataloader):\n",
    "    model.eval()\n",
    "    conf_score = torch.zeros([6]).to(device)\n",
    "    count = torch.zeros([6]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            inputs = data['input_ids'].squeeze(1).to(device, dtype = torch.long)\n",
    "            mask = data['mask'].squeeze(1).to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].squeeze(1).to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            logits = model(inputs, mask, token_type_ids)\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "\n",
    "            for i in range (len(targets)):\n",
    "                 \n",
    "                confidence = softmaxes[i][targets[i]]\n",
    "                conf_score[targets[i]] += confidence\n",
    "                count[targets[i]] += 1\n",
    "            conf_avg = conf_score/count\n",
    "    return conf_avg\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_wikiart_title_classi_new_ls_0.20.pth'))\n",
    "conf_score_train = get_conf_freq(model, train_data_loader)\n",
    "conf_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd811d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12b1c31",
   "metadata": {},
   "source": [
    "ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62001e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2694356143474579"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "TRAIN_BATCH_SIZE=32\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_wikiart_title_cls_transf.pth'))\n",
    "labels = []\n",
    "for i in range(len(test_data_loader.dataset)):\n",
    "    label = test_data_loader.dataset[i]['targets']\n",
    "    labels.append(label)\n",
    "labels = np.array(labels)\n",
    "\n",
    "class _ECELoss(nn.Module):\n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(_ECELoss, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece\n",
    "    \n",
    "def evaluation(model, testing_loader):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    pred_all = []\n",
    "    target_all = []\n",
    "    logits_list, labels_list = [], [] \n",
    "    with torch.no_grad():\n",
    "        for d in testing_loader:\n",
    "            input_ids = d[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask = d[\"mask\"].squeeze(1).to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask).logits\n",
    "            loss_val = loss_fn(outputs.squeeze(), targets.long())\n",
    "            predictions = torch.max(outputs, 1)[1].view(targets.size()).data\n",
    "            pred_all.append(outputs)\n",
    "            target_all.append(targets)\n",
    "            \n",
    "            f1 = f1_score(targets.data.cpu(), predictions.cpu(), average='macro')\n",
    "            num_corrects = (predictions == targets.data).float().sum()\n",
    "            acc = 100.0 * num_corrects / TRAIN_BATCH_SIZE\n",
    "            total_acc += acc.item()\n",
    "            total_loss += loss_val.item()\n",
    "            count += 1\n",
    "        logits_all = torch.cat(pred_all).cuda()\n",
    "        labels_all = torch.cat(target_all).cuda()\n",
    "    return total_acc/count,f1, logits_all, labels_all\n",
    "\n",
    "ece_criterion = _ECELoss().to(device)\n",
    "accuracy,total_f1, logits_all,labels_all = evaluation(model, test_data_loader)\n",
    "\n",
    "logits_all = logits_all.view(-1,6)\n",
    "labels_all = labels_all.view(-1)\n",
    "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
    "temperature_ece"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8b5931d",
   "metadata": {},
   "source": [
    "Reliability Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14075110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def ece_eval(preds, targets, n_bins=15, bg_cls = -1):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    confidences, predictions = np.max(preds,1), np.argmax(preds,1)\n",
    "    confidences, predictions = confidences[targets>bg_cls], predictions[targets>bg_cls]\n",
    "    accuracies = (predictions == targets[targets>bg_cls]) \n",
    "    Bm, acc, conf = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)\n",
    "    ece = 0.0\n",
    "    bin_idx = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n",
    "        #in_bin = in_bin[targets>backgound_class]\n",
    "        bin_size = np.sum(in_bin)\n",
    "        \n",
    "        Bm[bin_idx] = bin_size\n",
    "        if bin_size > 0:  \n",
    "            accuracy_in_bin = np.sum(accuracies[in_bin])\n",
    "            acc[bin_idx] = accuracy_in_bin / Bm[bin_idx]\n",
    "            confidence_in_bin = np.sum(confidences[in_bin])\n",
    "            conf[bin_idx] = confidence_in_bin / Bm[bin_idx]\n",
    "        bin_idx += 1\n",
    "        \n",
    "    ece_all = Bm * np.abs((acc - conf))/ Bm.sum()\n",
    "    ece = ece_all.sum() \n",
    "    return ece, acc, conf, Bm\n",
    "\n",
    "\n",
    "def get_reliability_diagram(conf_avg, acc_avg, legend=None, leg_idx=0, n_bins=10, fig=2):\n",
    "    plt.figure(fig)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--',label = 'Perfect Calibration')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(np.arange(0, 1.1, 1/n_bins))\n",
    "    plt.title('{}'.format(fig))\n",
    "    plt.plot(conf_avg[acc_avg>0],acc_avg[acc_avg>0], marker='.', label = legend)\n",
    "    plt.legend()\n",
    "    plt.savefig('ece_reliability_diag_{}.png'.format(fig),dpi=300)\n",
    "    \n",
    "logits_all =F.softmax(logits_all, dim=1).detach().cpu().numpy()\n",
    "labels_all =labels_all.detach().cpu().numpy()\n",
    "ece, acc, conf, Bm = ece_eval(logits_all, labels_all)\n",
    "get_reliability_diagram(conf, acc, legend='Reliability Diagram', fig='reliability_diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfce18ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.30918682 0.36304526\n",
      " 0.4433049  0.50715661 0.56093767 0.63684009 0.71442413 0.77148281\n",
      " 0.84196868 0.         0.        ] <class 'numpy.ndarray'>\n",
      "[0.         0.         0.         0.         0.33333333 0.18181818\n",
      " 0.5        0.75       0.66666667 0.57142857 0.7254902  0.55357143\n",
      " 0.70917574 0.         0.        ] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(conf, type(conf))\n",
    "print(acc,type(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bc8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
