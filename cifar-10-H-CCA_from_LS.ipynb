{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93e3009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/user-data/sa25729/cifar-10h\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/jcpeterson/cifar-10h\n",
    "%cd cifar-10h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "409ba526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import argparse\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "def seed_everything(seed=12):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "parser = argparse.ArgumentParser(description='CIFAR-10H Training')\n",
    "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
    "parser.add_argument('--batch_size', default=1024, type=int, help='batch size')\n",
    "parser.add_argument('--test_batch_size', default=2048, type=int, help='batch size')\n",
    "parser.add_argument('--num_epoch', default=100, type=int, help='epoch number')\n",
    "parser.add_argument('--num_classes', type=int, default=10, help='number classes')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585d2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets, ad) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets,conf_score)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9b4048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "class CIFAR10H(torchvision.datasets.CIFAR10):\n",
    "\n",
    "    def __init__(self, root,  rand_number=0, train=False, transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(CIFAR10H, self).__init__(root, train, transform, target_transform, download) \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.ad = np.load(os.path.join(root,'cifar10h-probs.npy'))\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "        ad = self.ad[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target, ad\n",
    "\n",
    "class CELossWithLS_CCA(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, classes= args.num_classes, smoothing=0.16, ignore_index=-1):\n",
    "        super(CELossWithLS_CCA, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.complement = 1.0 - smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target, conf_score):\n",
    "        with torch.no_grad():\n",
    "            new_smoothing  = self.smoothing - conf_score/10\n",
    "            new_complement = 1 - new_smoothing\n",
    "            oh_labels = F.one_hot(target.to(torch.int64), num_classes = self.cls).contiguous()\n",
    "            smoothen_ohlabel = oh_labels * new_complement + new_smoothing / self.cls\n",
    "        \n",
    "        logs = self.log_softmax(logits[target!=self.ignore_index])\n",
    "        return -torch.sum(logs * smoothen_ohlabel[target!=self.ignore_index], dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce9466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train samples: 10000 test samples: 50000\n",
      "epoch: 0  acc: 0.4281  best epoch: 0  best acc: 0.4281\n",
      "epoch: 1  acc: 0.5009  best epoch: 1  best acc: 0.5009\n",
      "epoch: 2  acc: 0.6504  best epoch: 2  best acc: 0.6504\n",
      "epoch: 3  acc: 0.6152  best epoch: 2  best acc: 0.6504\n",
      "epoch: 4  acc: 0.6039  best epoch: 2  best acc: 0.6504\n",
      "epoch: 5  acc: 0.6633  best epoch: 5  best acc: 0.6633\n",
      "epoch: 6  acc: 0.7363  best epoch: 6  best acc: 0.7363\n",
      "epoch: 7  acc: 0.7502  best epoch: 7  best acc: 0.7502\n",
      "epoch: 8  acc: 0.7327  best epoch: 7  best acc: 0.7502\n",
      "epoch: 9  acc: 0.7601  best epoch: 9  best acc: 0.7601\n",
      "epoch: 10  acc: 0.7657  best epoch: 10  best acc: 0.7657\n",
      "epoch: 11  acc: 0.7514  best epoch: 10  best acc: 0.7657\n",
      "epoch: 12  acc: 0.7539  best epoch: 10  best acc: 0.7657\n",
      "epoch: 13  acc: 0.7574  best epoch: 10  best acc: 0.7657\n",
      "epoch: 14  acc: 0.7617  best epoch: 10  best acc: 0.7657\n",
      "epoch: 15  acc: 0.7634  best epoch: 10  best acc: 0.7657\n",
      "epoch: 16  acc: 0.7389  best epoch: 10  best acc: 0.7657\n",
      "epoch: 17  acc: 0.7484  best epoch: 10  best acc: 0.7657\n",
      "epoch: 18  acc: 0.7631  best epoch: 10  best acc: 0.7657\n",
      "epoch: 19  acc: 0.7656  best epoch: 10  best acc: 0.7657\n",
      "epoch: 20  acc: 0.7694  best epoch: 20  best acc: 0.7694\n",
      "epoch: 21  acc: 0.7623  best epoch: 20  best acc: 0.7694\n",
      "epoch: 22  acc: 0.7712  best epoch: 22  best acc: 0.7712\n",
      "epoch: 23  acc: 0.7525  best epoch: 22  best acc: 0.7712\n",
      "epoch: 24  acc: 0.7771  best epoch: 24  best acc: 0.7771\n",
      "epoch: 25  acc: 0.7670  best epoch: 24  best acc: 0.7771\n",
      "epoch: 26  acc: 0.7408  best epoch: 24  best acc: 0.7771\n",
      "epoch: 27  acc: 0.7763  best epoch: 24  best acc: 0.7771\n",
      "epoch: 28  acc: 0.7742  best epoch: 24  best acc: 0.7771\n",
      "epoch: 29  acc: 0.7705  best epoch: 24  best acc: 0.7771\n",
      "epoch: 30  acc: 0.7696  best epoch: 24  best acc: 0.7771\n",
      "epoch: 31  acc: 0.7761  best epoch: 24  best acc: 0.7771\n",
      "epoch: 32  acc: 0.7726  best epoch: 24  best acc: 0.7771\n",
      "epoch: 33  acc: 0.7771  best epoch: 24  best acc: 0.7771\n",
      "epoch: 34  acc: 0.7780  best epoch: 34  best acc: 0.7780\n",
      "epoch: 35  acc: 0.7710  best epoch: 34  best acc: 0.7780\n",
      "epoch: 36  acc: 0.7790  best epoch: 36  best acc: 0.7790\n",
      "epoch: 37  acc: 0.7790  best epoch: 36  best acc: 0.7790\n",
      "epoch: 38  acc: 0.7829  best epoch: 38  best acc: 0.7829\n",
      "epoch: 39  acc: 0.7828  best epoch: 38  best acc: 0.7829\n",
      "epoch: 40  acc: 0.7719  best epoch: 38  best acc: 0.7829\n",
      "epoch: 41  acc: 0.7650  best epoch: 38  best acc: 0.7829\n",
      "epoch: 42  acc: 0.7720  best epoch: 38  best acc: 0.7829\n",
      "epoch: 43  acc: 0.7683  best epoch: 38  best acc: 0.7829\n",
      "epoch: 44  acc: 0.7741  best epoch: 38  best acc: 0.7829\n",
      "epoch: 45  acc: 0.7715  best epoch: 38  best acc: 0.7829\n",
      "epoch: 46  acc: 0.7710  best epoch: 38  best acc: 0.7829\n",
      "epoch: 47  acc: 0.7766  best epoch: 38  best acc: 0.7829\n",
      "epoch: 48  acc: 0.7820  best epoch: 38  best acc: 0.7829\n",
      "epoch: 49  acc: 0.7796  best epoch: 38  best acc: 0.7829\n",
      "epoch: 50  acc: 0.7779  best epoch: 38  best acc: 0.7829\n",
      "epoch: 51  acc: 0.7702  best epoch: 38  best acc: 0.7829\n",
      "epoch: 52  acc: 0.7789  best epoch: 38  best acc: 0.7829\n",
      "epoch: 53  acc: 0.7761  best epoch: 38  best acc: 0.7829\n",
      "epoch: 54  acc: 0.7688  best epoch: 38  best acc: 0.7829\n",
      "epoch: 55  acc: 0.7818  best epoch: 38  best acc: 0.7829\n",
      "epoch: 56  acc: 0.7732  best epoch: 38  best acc: 0.7829\n",
      "epoch: 57  acc: 0.7858  best epoch: 57  best acc: 0.7858\n",
      "epoch: 58  acc: 0.7908  best epoch: 58  best acc: 0.7908\n",
      "epoch: 59  acc: 0.7864  best epoch: 58  best acc: 0.7908\n",
      "epoch: 60  acc: 0.7777  best epoch: 58  best acc: 0.7908\n",
      "epoch: 61  acc: 0.7868  best epoch: 58  best acc: 0.7908\n",
      "epoch: 62  acc: 0.7725  best epoch: 58  best acc: 0.7908\n",
      "epoch: 63  acc: 0.7761  best epoch: 58  best acc: 0.7908\n",
      "epoch: 64  acc: 0.7855  best epoch: 58  best acc: 0.7908\n",
      "epoch: 65  acc: 0.7828  best epoch: 58  best acc: 0.7908\n",
      "epoch: 66  acc: 0.7785  best epoch: 58  best acc: 0.7908\n",
      "epoch: 67  acc: 0.7823  best epoch: 58  best acc: 0.7908\n",
      "epoch: 68  acc: 0.7867  best epoch: 58  best acc: 0.7908\n",
      "epoch: 69  acc: 0.7854  best epoch: 58  best acc: 0.7908\n",
      "epoch: 70  acc: 0.7827  best epoch: 58  best acc: 0.7908\n",
      "epoch: 71  acc: 0.7847  best epoch: 58  best acc: 0.7908\n",
      "epoch: 72  acc: 0.7862  best epoch: 58  best acc: 0.7908\n",
      "epoch: 73  acc: 0.7837  best epoch: 58  best acc: 0.7908\n",
      "epoch: 74  acc: 0.7844  best epoch: 58  best acc: 0.7908\n",
      "epoch: 75  acc: 0.7851  best epoch: 58  best acc: 0.7908\n",
      "epoch: 76  acc: 0.7808  best epoch: 58  best acc: 0.7908\n",
      "epoch: 77  acc: 0.7856  best epoch: 58  best acc: 0.7908\n",
      "epoch: 78  acc: 0.7860  best epoch: 58  best acc: 0.7908\n",
      "epoch: 79  acc: 0.7729  best epoch: 58  best acc: 0.7908\n",
      "epoch: 80  acc: 0.7840  best epoch: 58  best acc: 0.7908\n",
      "epoch: 81  acc: 0.7817  best epoch: 58  best acc: 0.7908\n",
      "epoch: 82  acc: 0.7866  best epoch: 58  best acc: 0.7908\n",
      "epoch: 83  acc: 0.7847  best epoch: 58  best acc: 0.7908\n",
      "epoch: 84  acc: 0.7838  best epoch: 58  best acc: 0.7908\n",
      "epoch: 85  acc: 0.7856  best epoch: 58  best acc: 0.7908\n",
      "epoch: 86  acc: 0.7815  best epoch: 58  best acc: 0.7908\n",
      "epoch: 87  acc: 0.7827  best epoch: 58  best acc: 0.7908\n",
      "epoch: 88  acc: 0.7865  best epoch: 58  best acc: 0.7908\n",
      "epoch: 89  acc: 0.7864  best epoch: 58  best acc: 0.7908\n",
      "epoch: 90  acc: 0.7758  best epoch: 58  best acc: 0.7908\n",
      "epoch: 91  acc: 0.7728  best epoch: 58  best acc: 0.7908\n",
      "epoch: 92  acc: 0.7838  best epoch: 58  best acc: 0.7908\n",
      "epoch: 93  acc: 0.7745  best epoch: 58  best acc: 0.7908\n",
      "epoch: 94  acc: 0.7738  best epoch: 58  best acc: 0.7908\n",
      "epoch: 95  acc: 0.7813  best epoch: 58  best acc: 0.7908\n",
      "epoch: 96  acc: 0.7758  best epoch: 58  best acc: 0.7908\n",
      "epoch: 97  acc: 0.7855  best epoch: 58  best acc: 0.7908\n",
      "epoch: 98  acc: 0.7820  best epoch: 58  best acc: 0.7908\n",
      "epoch: 99  acc: 0.7770  best epoch: 58  best acc: 0.7908\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "conf_score = torch.tensor([0.8265, 0.8410, 0.7920, 0.7833, 0.7851, 0.8231, 0.8496, 0.8212, 0.8126,\n",
    "        0.8997])\n",
    "conf_score = conf_score.to(device)\n",
    "mean_cifar10, std_cifar10 = (0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)\n",
    "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
    "            transforms.Normalize(mean_cifar10, std_cifar10), ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10, std_cifar10),])\n",
    "\n",
    "train_dataset = CIFAR10H(root='./data', train=False, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_test)\n",
    "print('train samples:',len(train_dataset), 'test samples:',len(test_dataset))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "model = models.resnet34(pretrained=True).to(device)\n",
    "model.fc = nn.Linear(model.fc.in_features, args.num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = CELossWithLS_CCA().to(device)\n",
    "\n",
    "best_epoch, best_acc = 0.0, 0\n",
    "for epoch in range(args.num_epoch):\n",
    "    train(model, train_loader, criterion, optimizer)\n",
    "    accuracy = test(model, test_loader)\n",
    "    if accuracy > best_acc:\n",
    "        patience = 0\n",
    "        best_acc = accuracy\n",
    "        best_epoch = epoch\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), 'best_model_cifar10h.pth_LS_cca_LS.tar')\n",
    "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
    "            epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727aa44",
   "metadata": {},
   "source": [
    "CCA: get confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4416870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train samples: 10000 test samples: 50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything()\n",
    "mean_cifar10, std_cifar10 = (0.5071, 0.4866, 0.4409), (0.2009, 0.1984, 0.2023)\n",
    "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(), transforms.ToTensor(),\n",
    "            transforms.Normalize(mean_cifar10, std_cifar10), ])\n",
    "transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10, std_cifar10),])\n",
    "\n",
    "train_dataset = CIFAR10H(root='./data', train=False, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_test)\n",
    "print('train samples:',len(train_dataset), 'test samples:',len(test_dataset))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "model = models.resnet34(pretrained=True).to(device)\n",
    "model.fc = nn.Linear(model.fc.in_features, args.num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=0.0001)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = CELossWithLS().to(device)\n",
    "model.load_state_dict(torch.load('best_model_cifar10h.pth_LS.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d42b3986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8265, 0.8410, 0.7920, 0.7833, 0.7851, 0.8231, 0.8496, 0.8212, 0.8126,\n",
       "        0.8997], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_cifar10h.pth_LS.tar'))\n",
    "def get_conf_freq (model, dataloader):\n",
    "    model.eval()\n",
    "    conf_score = torch.zeros([10]).to(device)\n",
    "    count = torch.zeros([10]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets, ad) in enumerate(dataloader):\n",
    "            inputs, targets, ad = inputs.to(device), targets.to(device), ad.to(device)\n",
    "            outputs = model(inputs)\n",
    "            softmaxes = F.softmax(outputs, dim=1)\n",
    "\n",
    "            for i in range (len(targets)):\n",
    "                confidence = softmaxes[i][targets[i]]\n",
    "                conf_score[targets[i]] += confidence\n",
    "                count[targets[i]] += 1\n",
    "            conf_avg = conf_score/count\n",
    "    return conf_avg\n",
    "\n",
    "conf_score = get_conf_freq(model, train_loader)\n",
    "conf_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ec590",
   "metadata": {},
   "source": [
    "Train the model with CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f727d99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  acc: 0.7211  best epoch: 0  best acc: 0.7211\n",
      "epoch: 1  acc: 0.7010  best epoch: 0  best acc: 0.7211\n",
      "epoch: 2  acc: 0.7431  best epoch: 2  best acc: 0.7431\n",
      "epoch: 3  acc: 0.7392  best epoch: 2  best acc: 0.7431\n",
      "epoch: 4  acc: 0.7680  best epoch: 4  best acc: 0.7680\n",
      "epoch: 5  acc: 0.7465  best epoch: 4  best acc: 0.7680\n",
      "epoch: 6  acc: 0.7725  best epoch: 6  best acc: 0.7725\n",
      "epoch: 7  acc: 0.7721  best epoch: 6  best acc: 0.7725\n",
      "epoch: 8  acc: 0.7481  best epoch: 6  best acc: 0.7725\n",
      "epoch: 9  acc: 0.7557  best epoch: 6  best acc: 0.7725\n"
     ]
    }
   ],
   "source": [
    "class CELossWithLS_conf(torch.nn.Module):\n",
    "    def __init__(self, classes= 10, smoothing=0.16, ignore_index=-1):\n",
    "        super(CELossWithLS_conf, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.complement = 1.0 - smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target, conf_score):\n",
    "        with torch.no_grad():\n",
    "            new_smoothing  = self.smoothing - conf_score/10\n",
    "            new_complement = 1 - new_smoothing\n",
    "            oh_labels = F.one_hot(target.to(torch.int64), num_classes = self.cls).contiguous()\n",
    "            smoothen_ohlabel = oh_labels * new_complement + new_smoothing / self.cls\n",
    "        \n",
    "        logs = self.log_softmax(logits[target!=self.ignore_index])\n",
    "        return -torch.sum(logs * smoothen_ohlabel[target!=self.ignore_index], dim=1).mean()\n",
    "    \n",
    "    \n",
    "def train_cca(model, trainloader, criterion, optimizer, conf_score):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets, ad) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets, conf_score)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "criterion_cca = CELossWithLS_conf().to(device)\n",
    "best_epoch, best_acc = 0.0, 0\n",
    "for epoch in range(args.num_epoch):\n",
    "    train_cca(model, train_loader, criterion_cca, optimizer, conf_score)\n",
    "    accuracy = test(model, test_loader)\n",
    "    if accuracy > best_acc:\n",
    "        patience = 0\n",
    "        best_acc = accuracy\n",
    "        best_epoch = epoch\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), 'best_model_cifar10h.pth_LS_cca.tar')\n",
    "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f}'.format(\n",
    "            epoch, accuracy, best_epoch, best_acc, optimizer.param_groups[0]['lr']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2606d084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
