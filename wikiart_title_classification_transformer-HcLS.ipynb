{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ef929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 30 15:49:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   19C    P8     9W / 250W |      0MiB / 23040MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P40           On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   41C    P0   132W / 250W |   2347MiB / 23040MiB |     96%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A     19005      C   ...sa25729/myenv/bin/python3     2345MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85afaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd30da48ae0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "import os\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def seed_everything(seed=12):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "RANDOM_SEED = 12\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95d6e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/processed_wikiart_title.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "786e6536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Token count')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "token_lens = []\n",
    "\n",
    "for txt in df.Title:\n",
    "    tokens = tokenizer.encode(txt, max_length=30)\n",
    "    token_lens.append(len(tokens))\n",
    "\n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 30]);\n",
    "plt.xlabel('Token count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb8f3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "class Wikiart_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, title, targets, tokenizer, max_len, ad):\n",
    "        self.title = title\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.ad = ad\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "  \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.title[item])\n",
    "        target = self.targets[item]\n",
    "        ad = np.array(self.ad[item][2:-2].split(' '), dtype = np.float32)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          review,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=True,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          \n",
    "          return_tensors='pt',\n",
    "        )\n",
    "        ids = encoding['input_ids']\n",
    "        mask = encoding['attention_mask']\n",
    "        \n",
    "        \n",
    "        return {\n",
    "          \n",
    "          'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "           'mask': torch.tensor(mask, dtype=torch.long),\n",
    "           'targets': torch.tensor(self.targets[item], dtype=torch.float),\n",
    "            'ad':  torch.tensor(ad, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39a3a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e91254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/data/user-data/sa25729/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'mask', 'targets', 'ad'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = Wikiart_Dataset(\n",
    "    title=df.Title.to_numpy(),\n",
    "    targets=df.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len,\n",
    "    ad = df.dist.to_numpy()\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2\n",
    "  )\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "data = next(iter(train_data_loader))\n",
    "data.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6689b2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "n_classes=6\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=n_classes).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64c1e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 6\n",
    "\n",
    "class CE_LS_MC(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, classes= num_classes, smoothing=0.13, ignore_index=-1):\n",
    "        super(CE_LS_MC, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.complement = 1.0 - smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target, ad):\n",
    "        with torch.no_grad():\n",
    "            new_smoothing  = self.smoothing + ad/100\n",
    "            new_complement = 1 - new_smoothing\n",
    "            oh_labels = F.one_hot(target.to(torch.int64), num_classes = self.cls).contiguous()\n",
    "            smoothen_ohlabel = oh_labels * new_complement + new_smoothing / self.cls\n",
    "        \n",
    "        logs = self.log_softmax(logits[target!=self.ignore_index])\n",
    "        return -torch.sum(logs * smoothen_ohlabel[target!=self.ignore_index], dim=1).mean()\n",
    "\n",
    "loss_fn = CE_LS_MC(classes = num_classes).to(device)\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = d[\"mask\"].squeeze(1).to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        ad = d['ad'].to(device)\n",
    "        outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets.long(),ad)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask = d[\"mask\"].squeeze(1).to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            ad = d['ad'].to(device)\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                ).logits\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            loss = loss_fn(outputs, targets.long(),ad)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60efb462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "Train loss 1.2418253873158427 accuracy 0.6120584652862363\n",
      "Val loss 1.158326614361543 accuracy 0.6626065773447015\n",
      "\n",
      "epoch: 0  acc: 0.6626  best epoch: 0  best acc: 0.6626 lr: 0.0000\n",
      "Epoch 2/100\n",
      "----------\n",
      "Train loss 0.9835557966556364 accuracy 0.7679658952496955\n",
      "Val loss 1.1938305841042445 accuracy 0.6528623629719853\n",
      "\n",
      "epoch: 1  acc: 0.6529  best epoch: 0  best acc: 0.6626 lr: 0.0000\n",
      "Epoch 3/100\n",
      "----------\n",
      "Train loss 0.8347233122992284 accuracy 0.8468331303288672\n",
      "Val loss 1.2288331618675818 accuracy 0.682095006090134\n",
      "\n",
      "epoch: 2  acc: 0.6821  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 4/100\n",
      "----------\n",
      "Train loss 0.738545997050202 accuracy 0.8937271619975639\n",
      "Val loss 1.2967822207854345 accuracy 0.658952496954933\n",
      "\n",
      "epoch: 3  acc: 0.6590  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 5/100\n",
      "----------\n",
      "Train loss 0.6846257778047358 accuracy 0.917783191230207\n",
      "Val loss 1.3391644817132216 accuracy 0.656516443361754\n",
      "\n",
      "epoch: 4  acc: 0.6565  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 6/100\n",
      "----------\n",
      "Train loss 0.6437622117764742 accuracy 0.9397076735688185\n",
      "Val loss 1.342377245426178 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 5  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 7/100\n",
      "----------\n",
      "Train loss 0.6171823317564807 accuracy 0.9521924482338611\n",
      "Val loss 1.3543893969975984 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 6  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 8/100\n",
      "----------\n",
      "Train loss 0.5984167890641295 accuracy 0.9625456760048721\n",
      "Val loss 1.4006713812167828 accuracy 0.6613885505481121\n",
      "\n",
      "epoch: 7  acc: 0.6614  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 9/100\n",
      "----------\n",
      "Train loss 0.5859077902673517 accuracy 0.9692448233861144\n",
      "Val loss 1.3938001531821032 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 8  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 10/100\n",
      "----------\n",
      "Train loss 0.5778119425171787 accuracy 0.9725943970767357\n",
      "Val loss 1.404493618469972 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 9  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 11/100\n",
      "----------\n",
      "Train loss 0.574849334156629 accuracy 0.9744214372716199\n",
      "Val loss 1.4262659549713135 accuracy 0.6674786845310596\n",
      "\n",
      "epoch: 10  acc: 0.6675  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 12/100\n",
      "----------\n",
      "Train loss 0.5738826972766987 accuracy 0.9741169305724725\n",
      "Val loss 1.4148790882183955 accuracy 0.6626065773447015\n",
      "\n",
      "epoch: 11  acc: 0.6626  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 13/100\n",
      "----------\n",
      "Train loss 0.5700149252576735 accuracy 0.9732034104750305\n",
      "Val loss 1.423402800009801 accuracy 0.6674786845310596\n",
      "\n",
      "epoch: 12  acc: 0.6675  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 14/100\n",
      "----------\n",
      "Train loss 0.5644811860566 accuracy 0.9765529841656516\n",
      "Val loss 1.4321329731207628 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 13  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 15/100\n",
      "----------\n",
      "Train loss 0.5639401228682509 accuracy 0.9783800243605358\n",
      "Val loss 1.4290885214622204 accuracy 0.6626065773447015\n",
      "\n",
      "epoch: 14  acc: 0.6626  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 16/100\n",
      "----------\n",
      "Train loss 0.5627257598256602 accuracy 0.9759439707673568\n",
      "Val loss 1.439773101073045 accuracy 0.6613885505481121\n",
      "\n",
      "epoch: 15  acc: 0.6614  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 17/100\n",
      "----------\n",
      "Train loss 0.5619614349985586 accuracy 0.9765529841656516\n",
      "Val loss 1.4286286739202647 accuracy 0.6674786845310596\n",
      "\n",
      "epoch: 16  acc: 0.6675  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 18/100\n",
      "----------\n",
      "Train loss 0.5607634360350452 accuracy 0.9783800243605358\n",
      "Val loss 1.441144810273097 accuracy 0.6638246041412911\n",
      "\n",
      "epoch: 17  acc: 0.6638  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 19/100\n",
      "----------\n",
      "Train loss 0.5604623729742847 accuracy 0.976857490864799\n",
      "Val loss 1.4448880874193633 accuracy 0.6613885505481121\n",
      "\n",
      "epoch: 18  acc: 0.6614  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 20/100\n",
      "----------\n",
      "Train loss 0.5582386868671306 accuracy 0.9795980511571254\n",
      "Val loss 1.4470311540823717 accuracy 0.6626065773447015\n",
      "\n",
      "epoch: 19  acc: 0.6626  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 21/100\n",
      "----------\n",
      "Train loss 0.5599574967495446 accuracy 0.9777710109622411\n",
      "Val loss 1.4387733661211455 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 20  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 22/100\n",
      "----------\n",
      "Train loss 0.5581672353651917 accuracy 0.9786845310596832\n",
      "Val loss 1.4289444226485033 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 21  acc: 0.6736  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 23/100\n",
      "----------\n",
      "Train loss 0.5577975870336144 accuracy 0.9780755176613884\n",
      "Val loss 1.4390312754190886 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 22  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 24/100\n",
      "----------\n",
      "Train loss 0.5574637518345731 accuracy 0.979293544457978\n",
      "Val loss 1.437021989088792 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 23  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 25/100\n",
      "----------\n",
      "Train loss 0.5579300806360338 accuracy 0.9771619975639464\n",
      "Val loss 1.4340652227401733 accuracy 0.6772228989037758\n",
      "\n",
      "epoch: 24  acc: 0.6772  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 26/100\n",
      "----------\n",
      "Train loss 0.5579062353060084 accuracy 0.976857490864799\n",
      "Val loss 1.4420017622984374 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 25  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 27/100\n",
      "----------\n",
      "Train loss 0.5577755958131216 accuracy 0.976857490864799\n",
      "Val loss 1.4405248417304113 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 26  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 28/100\n",
      "----------\n",
      "Train loss 0.5566735418097487 accuracy 0.9771619975639464\n",
      "Val loss 1.4532448282608619 accuracy 0.6638246041412911\n",
      "\n",
      "epoch: 27  acc: 0.6638  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 29/100\n",
      "----------\n",
      "Train loss 0.5562067616333082 accuracy 0.9786845310596832\n",
      "Val loss 1.4496152102947235 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 28  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 30/100\n",
      "----------\n",
      "Train loss 0.5560377072362066 accuracy 0.9783800243605358\n",
      "Val loss 1.449211033490988 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 29  acc: 0.6760  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 31/100\n",
      "----------\n",
      "Train loss 0.5566808500336212 accuracy 0.9765529841656516\n",
      "Val loss 1.4630248592450068 accuracy 0.6613885505481121\n",
      "\n",
      "epoch: 30  acc: 0.6614  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 32/100\n",
      "----------\n",
      "Train loss 0.555807356695527 accuracy 0.979293544457978\n",
      "Val loss 1.463354427080888 accuracy 0.6638246041412911\n",
      "\n",
      "epoch: 31  acc: 0.6638  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 33/100\n",
      "----------\n",
      "Train loss 0.5558240350010326 accuracy 0.976857490864799\n",
      "Val loss 1.4603450986055226 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 32  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 34/100\n",
      "----------\n",
      "Train loss 0.5554130852801128 accuracy 0.9786845310596832\n",
      "Val loss 1.449200770029655 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 33  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 35/100\n",
      "----------\n",
      "Train loss 0.5556359973925988 accuracy 0.9780755176613884\n",
      "Val loss 1.4490415362211375 accuracy 0.6674786845310596\n",
      "\n",
      "epoch: 34  acc: 0.6675  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 36/100\n",
      "----------\n",
      "Train loss 0.5553548654306282 accuracy 0.979293544457978\n",
      "Val loss 1.4347447111056402 accuracy 0.6747868453105969\n",
      "\n",
      "epoch: 35  acc: 0.6748  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 37/100\n",
      "----------\n",
      "Train loss 0.5553960910121214 accuracy 0.9789890377588306\n",
      "Val loss 1.4476280441650977 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 36  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 38/100\n",
      "----------\n",
      "Train loss 0.5556284333895711 accuracy 0.9786845310596832\n",
      "Val loss 1.449906431711637 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 37  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 39/100\n",
      "----------\n",
      "Train loss 0.5560916025661727 accuracy 0.979293544457978\n",
      "Val loss 1.451543459525475 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 38  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 40/100\n",
      "----------\n",
      "Train loss 0.5556837667539282 accuracy 0.9783800243605358\n",
      "Val loss 1.4514249654916616 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 39  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 41/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5546526133435444 accuracy 0.9799025578562728\n",
      "Val loss 1.4530238967675428 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 40  acc: 0.6760  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 42/100\n",
      "----------\n",
      "Train loss 0.5552392439934813 accuracy 0.9780755176613884\n",
      "Val loss 1.4322846944515522 accuracy 0.6772228989037758\n",
      "\n",
      "epoch: 41  acc: 0.6772  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 43/100\n",
      "----------\n",
      "Train loss 0.5556409347404554 accuracy 0.9780755176613884\n",
      "Val loss 1.4618223492915814 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 42  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 44/100\n",
      "----------\n",
      "Train loss 0.5546157527895808 accuracy 0.9799025578562728\n",
      "Val loss 1.4506928370549128 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 43  acc: 0.6784  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 45/100\n",
      "----------\n",
      "Train loss 0.5557021730154463 accuracy 0.9783800243605358\n",
      "Val loss 1.4773919261418855 accuracy 0.6626065773447015\n",
      "\n",
      "epoch: 44  acc: 0.6626  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 46/100\n",
      "----------\n",
      "Train loss 0.5544273019994347 accuracy 0.9799025578562728\n",
      "Val loss 1.4649943984471834 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 45  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 47/100\n",
      "----------\n",
      "Train loss 0.5545472213365499 accuracy 0.9783800243605358\n",
      "Val loss 1.443140332515423 accuracy 0.6784409257003654\n",
      "\n",
      "epoch: 46  acc: 0.6784  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 48/100\n",
      "----------\n",
      "Train loss 0.5539780722081082 accuracy 0.979293544457978\n",
      "Val loss 1.4595689223362849 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 47  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 49/100\n",
      "----------\n",
      "Train loss 0.5548329775773205 accuracy 0.9795980511571254\n",
      "Val loss 1.469868045586806 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 48  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 50/100\n",
      "----------\n",
      "Train loss 0.5545325828987417 accuracy 0.9786845310596832\n",
      "Val loss 1.468783410695883 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 49  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 51/100\n",
      "----------\n",
      "Train loss 0.5541521860557852 accuracy 0.9789890377588306\n",
      "Val loss 1.477050286072951 accuracy 0.6638246041412911\n",
      "\n",
      "epoch: 50  acc: 0.6638  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 52/100\n",
      "----------\n",
      "Train loss 0.5537106169080271 accuracy 0.9799025578562728\n",
      "Val loss 1.465647958792173 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 51  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 53/100\n",
      "----------\n",
      "Train loss 0.5534785916504351 accuracy 0.9783800243605358\n",
      "Val loss 1.4706969490418067 accuracy 0.6638246041412911\n",
      "\n",
      "epoch: 52  acc: 0.6638  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 54/100\n",
      "----------\n",
      "Train loss 0.5553748318292562 accuracy 0.9799025578562728\n",
      "Val loss 1.462660862849309 accuracy 0.6638246041412911\n",
      "\n",
      "epoch: 53  acc: 0.6638  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 55/100\n",
      "----------\n",
      "Train loss 0.5541059056532036 accuracy 0.9786845310596832\n",
      "Val loss 1.4754951000213623 accuracy 0.6650426309378806\n",
      "\n",
      "epoch: 54  acc: 0.6650  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 56/100\n",
      "----------\n",
      "Train loss 0.5537091910260395 accuracy 0.9805115712545676\n",
      "Val loss 1.4687847586778493 accuracy 0.6626065773447015\n",
      "\n",
      "epoch: 55  acc: 0.6626  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 57/100\n",
      "----------\n",
      "Train loss 0.5537473750345915 accuracy 0.9799025578562728\n",
      "Val loss 1.4794939848092885 accuracy 0.6650426309378806\n",
      "\n",
      "epoch: 56  acc: 0.6650  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 58/100\n",
      "----------\n",
      "Train loss 0.5538156455002942 accuracy 0.979293544457978\n",
      "Val loss 1.4621331783441396 accuracy 0.6638246041412911\n",
      "\n",
      "epoch: 57  acc: 0.6638  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 59/100\n",
      "----------\n",
      "Train loss 0.5540361821072773 accuracy 0.9786845310596832\n",
      "Val loss 1.4827379355063806 accuracy 0.6638246041412911\n",
      "\n",
      "epoch: 58  acc: 0.6638  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 60/100\n",
      "----------\n",
      "Train loss 0.5537230557608372 accuracy 0.9786845310596832\n",
      "Val loss 1.4716637088702276 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 59  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 61/100\n",
      "----------\n",
      "Train loss 0.5541921234825282 accuracy 0.9795980511571254\n",
      "Val loss 1.465437426016881 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 60  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 62/100\n",
      "----------\n",
      "Train loss 0.5538243159507085 accuracy 0.979293544457978\n",
      "Val loss 1.4761511041567876 accuracy 0.6650426309378806\n",
      "\n",
      "epoch: 61  acc: 0.6650  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 63/100\n",
      "----------\n",
      "Train loss 0.55329281381033 accuracy 0.9799025578562728\n",
      "Val loss 1.4581806522149305 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 62  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 64/100\n",
      "----------\n",
      "Train loss 0.5533621727841572 accuracy 0.9799025578562728\n",
      "Val loss 1.44179017497943 accuracy 0.6808769792935444\n",
      "\n",
      "epoch: 63  acc: 0.6809  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 65/100\n",
      "----------\n",
      "Train loss 0.5537304114369512 accuracy 0.9802070645554202\n",
      "Val loss 1.4552909089968755 accuracy 0.6760048721071863\n",
      "\n",
      "epoch: 64  acc: 0.6760  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 66/100\n",
      "----------\n",
      "Train loss 0.5538191372908435 accuracy 0.9795980511571254\n",
      "Val loss 1.4730846652617822 accuracy 0.6638246041412911\n",
      "\n",
      "epoch: 65  acc: 0.6638  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 67/100\n",
      "----------\n",
      "Train loss 0.5531915345238251 accuracy 0.9789890377588306\n",
      "Val loss 1.4664728824908917 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 66  acc: 0.6724  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 68/100\n",
      "----------\n",
      "Train loss 0.5530919513656097 accuracy 0.9814250913520097\n",
      "Val loss 1.4685447032635028 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 67  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 69/100\n",
      "----------\n",
      "Train loss 0.5536294602653355 accuracy 0.980816077953715\n",
      "Val loss 1.4791485896477332 accuracy 0.6613885505481121\n",
      "\n",
      "epoch: 68  acc: 0.6614  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 70/100\n",
      "----------\n",
      "Train loss 0.5532209207710711 accuracy 0.979293544457978\n",
      "Val loss 1.474979859132033 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 69  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 71/100\n",
      "----------\n",
      "Train loss 0.5534983387271177 accuracy 0.9799025578562728\n",
      "Val loss 1.471365791100722 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 70  acc: 0.6724  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 72/100\n",
      "----------\n",
      "Train loss 0.5531197630085991 accuracy 0.9795980511571254\n",
      "Val loss 1.4697571626076331 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 71  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 73/100\n",
      "----------\n",
      "Train loss 0.5530563795450821 accuracy 0.9802070645554202\n",
      "Val loss 1.471368175286513 accuracy 0.6674786845310596\n",
      "\n",
      "epoch: 72  acc: 0.6675  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 74/100\n",
      "----------\n",
      "Train loss 0.5532198970757641 accuracy 0.979293544457978\n",
      "Val loss 1.4679332834023695 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 73  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 75/100\n",
      "----------\n",
      "Train loss 0.5531256551881438 accuracy 0.9799025578562728\n",
      "Val loss 1.4626600329692547 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 74  acc: 0.6724  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 76/100\n",
      "----------\n",
      "Train loss 0.5529434362661492 accuracy 0.9817295980511571\n",
      "Val loss 1.4624297756415148 accuracy 0.6796589524969548\n",
      "\n",
      "epoch: 75  acc: 0.6797  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 77/100\n",
      "----------\n",
      "Train loss 0.5535551523699344 accuracy 0.9783800243605358\n",
      "Val loss 1.4646687255455897 accuracy 0.6772228989037758\n",
      "\n",
      "epoch: 76  acc: 0.6772  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 78/100\n",
      "----------\n",
      "Train loss 0.5533888936042786 accuracy 0.9805115712545676\n",
      "Val loss 1.474196507380559 accuracy 0.6686967113276492\n",
      "\n",
      "epoch: 77  acc: 0.6687  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 79/100\n",
      "----------\n",
      "Train loss 0.553543421828631 accuracy 0.9799025578562728\n",
      "Val loss 1.4754689152424152 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 78  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 80/100\n",
      "----------\n",
      "Train loss 0.5534295214032664 accuracy 0.9786845310596832\n",
      "Val loss 1.4678485645697668 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 79  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 81/100\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5529927501400698 accuracy 0.9799025578562728\n",
      "Val loss 1.4733817669061513 accuracy 0.6662606577344701\n",
      "\n",
      "epoch: 80  acc: 0.6663  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 82/100\n",
      "----------\n",
      "Train loss 0.5530260106892262 accuracy 0.9786845310596832\n",
      "Val loss 1.4742803023411677 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 81  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 83/100\n",
      "----------\n",
      "Train loss 0.5527189645952392 accuracy 0.9805115712545676\n",
      "Val loss 1.4696443310150733 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 82  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 84/100\n",
      "----------\n",
      "Train loss 0.5528059358735686 accuracy 0.9811205846528623\n",
      "Val loss 1.4630747781350062 accuracy 0.6735688185140073\n",
      "\n",
      "epoch: 83  acc: 0.6736  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 85/100\n",
      "----------\n",
      "Train loss 0.5529246880012808 accuracy 0.9811205846528623\n",
      "Val loss 1.4671508348905122 accuracy 0.6772228989037758\n",
      "\n",
      "epoch: 84  acc: 0.6772  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 86/100\n",
      "----------\n",
      "Train loss 0.5530109608057633 accuracy 0.9820341047503045\n",
      "Val loss 1.478706446977762 accuracy 0.6650426309378806\n",
      "\n",
      "epoch: 85  acc: 0.6650  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 87/100\n",
      "----------\n",
      "Train loss 0.5524792080944024 accuracy 0.9805115712545676\n",
      "Val loss 1.4746517539024353 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 86  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 88/100\n",
      "----------\n",
      "Train loss 0.5531399695618638 accuracy 0.9786845310596832\n",
      "Val loss 1.4721287213839018 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 87  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 89/100\n",
      "----------\n",
      "Train loss 0.5530387909667006 accuracy 0.9814250913520097\n",
      "Val loss 1.4693338458354657 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 88  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 90/100\n",
      "----------\n",
      "Train loss 0.5525394180445995 accuracy 0.9814250913520097\n",
      "Val loss 1.471687798316662 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 89  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 91/100\n",
      "----------\n",
      "Train loss 0.552677571773529 accuracy 0.9805115712545676\n",
      "Val loss 1.4695774821134715 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 90  acc: 0.6724  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 92/100\n",
      "----------\n",
      "Train loss 0.5524525596100149 accuracy 0.9817295980511571\n",
      "Val loss 1.4693560875379121 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 91  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 93/100\n",
      "----------\n",
      "Train loss 0.553091301501376 accuracy 0.9811205846528623\n",
      "Val loss 1.467322853895334 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 92  acc: 0.6724  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 94/100\n",
      "----------\n",
      "Train loss 0.5528085850974889 accuracy 0.9811205846528623\n",
      "Val loss 1.4686291447052588 accuracy 0.6723507917174177\n",
      "\n",
      "epoch: 93  acc: 0.6724  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 95/100\n",
      "----------\n",
      "Train loss 0.5522281637469542 accuracy 0.9820341047503045\n",
      "Val loss 1.4710150682009184 accuracy 0.6711327649208282\n",
      "\n",
      "epoch: 94  acc: 0.6711  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 96/100\n",
      "----------\n",
      "Train loss 0.5526700864717798 accuracy 0.9805115712545676\n",
      "Val loss 1.4714482610042279 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 95  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 97/100\n",
      "----------\n",
      "Train loss 0.5523953119527947 accuracy 0.9820341047503045\n",
      "Val loss 1.4698692285097563 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 96  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 98/100\n",
      "----------\n",
      "Train loss 0.55262304046779 accuracy 0.9820341047503045\n",
      "Val loss 1.4701069134932299 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 97  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 99/100\n",
      "----------\n",
      "Train loss 0.5523547593829701 accuracy 0.9811205846528623\n",
      "Val loss 1.469967204790849 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 98  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n",
      "Epoch 100/100\n",
      "----------\n",
      "Train loss 0.5529896177134468 accuracy 0.980816077953715\n",
      "Val loss 1.470123758682838 accuracy 0.6699147381242387\n",
      "\n",
      "epoch: 99  acc: 0.6699  best epoch: 2  best acc: 0.6821 lr: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed_everything()\n",
    "EPOCHS = 100\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "            \n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model,train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train))\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(model,test_data_loader,loss_fn, device, len(df_test))\n",
    "\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_wikiart_title_cls_transf_HcLS.pth')\n",
    "        best_accuracy = val_acc\n",
    "        best_epoch = epoch\n",
    "    print('epoch: {}  acc: {:.4f}  best epoch: {}  best acc: {:.4f} lr: {:.4f}'.format(\n",
    "            epoch, val_acc, best_epoch, best_accuracy,optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971c4d4",
   "metadata": {},
   "source": [
    "Confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29edfea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8871, 0.8848, 0.8912, 0.8898, 0.7549, 0.8843], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_conf_freq (model, dataloader):\n",
    "    model.eval()\n",
    "    conf_score = torch.zeros([6]).to(device)\n",
    "    count = torch.zeros([6]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for d in dataloader:\n",
    "            input_ids = d[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask = d[\"mask\"].squeeze(1).to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                ).logits\n",
    "            softmaxes = F.softmax(outputs, dim=1)\n",
    "\n",
    "            for i in range (len(targets)):\n",
    "                 \n",
    "                confidence = softmaxes[i][targets[i].long()]\n",
    "                conf_score[targets[i].long()] += confidence\n",
    "                count[targets[i].long()] += 1\n",
    "            conf_avg = conf_score/count\n",
    "    return conf_avg\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_wikiart_title_cls_transf_LS_0.13.pth'))\n",
    "conf_score_train = get_conf_freq(model, train_data_loader)\n",
    "conf_score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd811d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12b1c31",
   "metadata": {},
   "source": [
    "ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62001e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09203013777732849"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score\n",
    "TRAIN_BATCH_SIZE=32\n",
    "\n",
    "model.load_state_dict(torch.load('best_model_wikiart_title_cls_transf_HcLS.pth'))\n",
    "\n",
    "labels = []\n",
    "for i in range(len(test_data_loader.dataset)):\n",
    "    label = test_data_loader.dataset[i]['targets']\n",
    "    labels.append(label)\n",
    "labels = np.array(labels)\n",
    "\n",
    "class _ECELoss(nn.Module):\n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(_ECELoss, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece\n",
    "    \n",
    "def evaluation(model, testing_loader):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    pred_all = []\n",
    "    target_all = []\n",
    "    logits_list, labels_list = [], [] \n",
    "    with torch.no_grad():\n",
    "        for d in testing_loader:\n",
    "            input_ids = d[\"input_ids\"].squeeze(1).to(device)\n",
    "            attention_mask = d[\"mask\"].squeeze(1).to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            ad = d['ad'].to(device)\n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask).logits\n",
    "            loss_val = loss_fn(outputs.squeeze(), targets.long(), ad)\n",
    "            predictions = torch.max(outputs, 1)[1].view(targets.size()).data\n",
    "            pred_all.append(outputs)\n",
    "            target_all.append(targets)\n",
    "            \n",
    "            f1 = f1_score(targets.data.cpu(), predictions.cpu(), average='macro')\n",
    "            num_corrects = (predictions == targets.data).float().sum()\n",
    "            acc = 100.0 * num_corrects / TRAIN_BATCH_SIZE\n",
    "            total_acc += acc.item()\n",
    "            total_loss += loss_val.item()\n",
    "            count += 1\n",
    "        logits_all = torch.cat(pred_all).cuda()\n",
    "        labels_all = torch.cat(target_all).cuda()\n",
    "    return total_acc/count,f1, logits_all, labels_all\n",
    "ece_criterion = _ECELoss().to(device)\n",
    "accuracy,total_f1, logits_all,labels_all = evaluation(model, test_data_loader)\n",
    "\n",
    "logits_all = logits_all.view(-1,6)\n",
    "labels_all = labels_all.view(-1)\n",
    "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
    "temperature_ece"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8b5931d",
   "metadata": {},
   "source": [
    "Reliability Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14075110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def ece_eval(preds, targets, n_bins=15, bg_cls = -1):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    confidences, predictions = np.max(preds,1), np.argmax(preds,1)\n",
    "    confidences, predictions = confidences[targets>bg_cls], predictions[targets>bg_cls]\n",
    "    accuracies = (predictions == targets[targets>bg_cls]) \n",
    "    Bm, acc, conf = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)\n",
    "    ece = 0.0\n",
    "    bin_idx = 0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)\n",
    "        #in_bin = in_bin[targets>backgound_class]\n",
    "        bin_size = np.sum(in_bin)\n",
    "        \n",
    "        Bm[bin_idx] = bin_size\n",
    "        if bin_size > 0:  \n",
    "            accuracy_in_bin = np.sum(accuracies[in_bin])\n",
    "            acc[bin_idx] = accuracy_in_bin / Bm[bin_idx]\n",
    "            confidence_in_bin = np.sum(confidences[in_bin])\n",
    "            conf[bin_idx] = confidence_in_bin / Bm[bin_idx]\n",
    "        bin_idx += 1\n",
    "        \n",
    "    ece_all = Bm * np.abs((acc - conf))/ Bm.sum()\n",
    "    ece = ece_all.sum() \n",
    "    return ece, acc, conf, Bm\n",
    "\n",
    "\n",
    "def get_reliability_diagram(conf_avg, acc_avg, legend=None, leg_idx=0, n_bins=10, fig=2):\n",
    "    plt.figure(fig)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--',label = 'Perfect Calibration')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(np.arange(0, 1.1, 1/n_bins))\n",
    "    plt.title('{}'.format(fig))\n",
    "    plt.plot(conf_avg[acc_avg>0],acc_avg[acc_avg>0], marker='.', label = legend)\n",
    "    plt.legend()\n",
    "    plt.savefig('ece_reliability_diag_{}.png'.format(fig),dpi=300)\n",
    "    \n",
    "logits_all =F.softmax(logits_all, dim=1).detach().cpu().numpy()\n",
    "labels_all =labels_all.detach().cpu().numpy()\n",
    "ece, acc, conf, Bm = ece_eval(logits_all, labels_all)\n",
    "get_reliability_diagram(conf, acc, legend='Reliability Diagram', fig='reliability_diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfce18ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.30088486 0.36884165\n",
      " 0.43975141 0.50331284 0.56652616 0.634132   0.7032699  0.76678306\n",
      " 0.84131735 0.88679969 0.        ] <class 'numpy.ndarray'>\n",
      "[0.         0.         0.         0.         0.42857143 0.35714286\n",
      " 0.48571429 0.40384615 0.66666667 0.64       0.52380952 0.61111111\n",
      " 0.74619289 0.81818182 0.        ] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(conf, type(conf))\n",
    "print(acc,type(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bc8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
